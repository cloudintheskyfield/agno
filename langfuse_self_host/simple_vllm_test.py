"""
ç®€åŒ–ç‰ˆ VLLM æµ‹è¯• - æ— éœ€ Langfuse é…ç½®
å¦‚æœä½ æš‚æ—¶ä¸æƒ³é…ç½® Langfuseï¼Œå¯ä»¥å…ˆç”¨è¿™ä¸ªè„šæœ¬æµ‹è¯• VLLM æ¨¡å‹
"""

from openai import OpenAI
import time
import json

class SimpleVLLMTest:
    def __init__(self):
        self.client = OpenAI(
            api_key="EMPTY",  # VLLM ä¸éœ€è¦çœŸå®çš„ API key
            base_url='http://223.109.239.14:10026/v1/'
        )
        self.model_id = 'Qwen3-Omni-Thinking'
        
        print("ğŸš€ ç®€åŒ–ç‰ˆ VLLM æµ‹è¯•å®¢æˆ·ç«¯")
        print(f"   æ¨¡å‹: {self.model_id}")
        print(f"   æœåŠ¡åœ°å€: http://223.109.239.14:10026/v1/")
        print("   çŠ¶æ€: æ— éœ€ä»»ä½•ä»˜è´¹å¯†é’¥ âœ…")
        print()
    
    def simple_chat(self, prompt: str):
        """ç®€å•çš„èŠå¤©æµ‹è¯•"""
        print(f"ğŸ“ å‘é€æ¶ˆæ¯: {prompt[:50]}...")
        
        start_time = time.time()
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_id,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=32768,
                temperature=1.0
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            content = response.choices[0].message.content
            usage = response.usage
            
            print(f"âœ… æˆåŠŸå“åº” (è€—æ—¶: {duration:.2f}s)")
            print(f"ğŸ“Š Token ä½¿ç”¨: è¾“å…¥ {usage.prompt_tokens}, è¾“å‡º {usage.completion_tokens}")
            print(f"ğŸ“„ å›å¤: {content[:200]}...")
            print()
            
            return {
                "content": content,
                "duration": duration,
                "usage": usage.model_dump()
            }
            
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def run_tests(self):
        """è¿è¡Œä¸€ç³»åˆ—æµ‹è¯•"""
        test_prompts = [
            "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
            "è¯·ç”¨ä¸­æ–‡è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "What are the main advantages of using VLLM?",
            "è¯·å†™ä¸€ä¸ªç®€å•çš„ Python å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚"
        ]
        
        print(f"ğŸ§ª å¼€å§‹è¿è¡Œ {len(test_prompts)} ä¸ªæµ‹è¯•...")
        print("=" * 60)
        
        results = []
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\nğŸ”¸ æµ‹è¯• {i}/{len(test_prompts)}")
            result = self.simple_chat(prompt)
            
            if result:
                results.append({
                    "test_number": i,
                    "prompt": prompt,
                    "success": True,
                    **result
                })
            else:
                results.append({
                    "test_number": i,
                    "prompt": prompt,
                    "success": False
                })
            
            # çŸ­æš‚å»¶è¿Ÿ
            time.sleep(0.5)
        
        # æ˜¾ç¤ºæµ‹è¯•æ‘˜è¦
        self.show_summary(results)
        
        return results
    
    def show_summary(self, results):
        """æ˜¾ç¤ºæµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•æ‘˜è¦")
        print("=" * 60)
        
        successful = [r for r in results if r.get("success", False)]
        failed = [r for r in results if not r.get("success", False)]
        
        print(f"âœ… æˆåŠŸ: {len(successful)}/{len(results)}")
        print(f"âŒ å¤±è´¥: {len(failed)}/{len(results)}")
        
        if successful:
            avg_duration = sum(r["duration"] for r in successful) / len(successful)
            total_input_tokens = sum(r["usage"]["prompt_tokens"] for r in successful)
            total_output_tokens = sum(r["usage"]["completion_tokens"] for r in successful)
            
            print(f"â±ï¸ å¹³å‡å“åº”æ—¶é—´: {avg_duration:.2f}s")
            print(f"ğŸ“Š æ€»è¾“å…¥ Token: {total_input_tokens}")
            print(f"ğŸ“Š æ€»è¾“å‡º Token: {total_output_tokens}")
            print(f"ğŸ’° æˆæœ¬: å®Œå…¨å…è´¹ ğŸ‰")
        
        print("\nğŸ’¡ æç¤º:")
        print("   - è¿™ä¸ªæµ‹è¯•å®Œå…¨å…è´¹ï¼Œæ— éœ€ä»»ä½•ä»˜è´¹å¯†é’¥")
        print("   - å¦‚æœæƒ³è¦è¯¦ç»†ç›‘æ§ï¼Œå¯ä»¥é…ç½® Langfuseï¼ˆä¹Ÿæ˜¯å…è´¹çš„ï¼‰")
        print("   - æ‰€æœ‰æ•°æ®éƒ½åœ¨ä½ æœ¬åœ°ï¼Œå®Œå…¨ç§æœ‰å®‰å…¨")

def main():
    print("ğŸ¯ VLLM æ¨¡å‹å…è´¹æµ‹è¯•")
    print("=" * 60)
    print("âœ… æ— éœ€ä»»ä½•ä»˜è´¹å¯†é’¥")
    print("âœ… å®Œå…¨å…è´¹ä½¿ç”¨")
    print("âœ… æ•°æ®å®Œå…¨ç§æœ‰")
    print()
    
    # åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯
    tester = SimpleVLLMTest()
    
    # è¿è¡Œæµ‹è¯•
    results = tester.run_tests()
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ”— ç›¸å…³ä¿¡æ¯:")
    print("   - VLLM æœåŠ¡: http://223.109.239.14:10026")
    print("   - æ¨¡å‹: Qwen3-Omni-Thinking")
    print("   - è´¹ç”¨: å®Œå…¨å…è´¹ ğŸ’°")
    
    return results

if __name__ == "__main__":
    main()
