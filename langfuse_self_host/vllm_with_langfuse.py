"""
VLLM æ¨¡å‹è°ƒç”¨ä¸ Langfuse é›†æˆç¤ºä¾‹
è®°å½•æ¨¡å‹è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¾“å…¥ã€è¾“å‡ºã€å»¶è¿Ÿç­‰
"""

import os
from dotenv import load_dotenv
from langfuse import Langfuse
from langfuse import observe
from openai import OpenAI
import time
from typing import Optional, Dict, Any

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class VLLMWithLangfuse:
    def __init__(self,
                 model_id: str = 'Qwen3-Omni-Thinking',
                 base_url: str = 'http://223.109.239.14:10026/v1/',
                 max_tokens: int = 32768,
                 temperature: float = 1.0):
        """
        åˆå§‹åŒ– VLLM å®¢æˆ·ç«¯å’Œ Langfuse
        """
        self.model_id = model_id
        self.base_url = base_url
        self.max_tokens = max_tokens
        self.temperature = temperature

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äº VLLMï¼‰
        self.client = OpenAI(
            api_key="EMPTY",  # VLLM ä¸éœ€è¦çœŸå®çš„ API key
            base_url=base_url
        )

        # åˆå§‹åŒ– Langfuse
        self.langfuse = Langfuse(
            public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
            secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
            host=os.getenv("LANGFUSE_HOST", "http://localhost:3000")
        )

        print(f"âœ… VLLM å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ¨¡å‹: {self.model_id}")
        print(f"   åŸºç¡€URL: {self.base_url}")
        print(f"   Langfuse Host: {os.getenv('LANGFUSE_HOST', 'http://localhost:3000')}")

    @observe(name="vllm_chat_completion")
    def chat_completion(self,
                       messages: list,
                       stream: bool = False,
                       session_id: Optional[str] = None,
                       user_id: Optional[str] = None,
                       metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        æ‰§è¡ŒèŠå¤©å®Œæˆè¯·æ±‚å¹¶è®°å½•åˆ° Langfuse
        """
        start_time = time.time()

        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            request_params = {
                "model": self.model_id,
                "messages": messages,
                "max_tokens": self.max_tokens,
                "temperature": self.temperature,
                "stream": stream
            }

            # è°ƒç”¨ VLLM API
            response = self.client.chat.completions.create(**request_params)

            end_time = time.time()
            latency = end_time - start_time

            # å¤„ç†å“åº”
            if stream:
                # æµå¼å“åº”å¤„ç†
                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield content

                response_data = {
                    "content": full_response,
                    "model": self.model_id,
                    "usage": {"completion_tokens": len(full_response.split())},
                    "latency": latency
                }
            else:
                # éæµå¼å“åº”å¤„ç†
                response_data = {
                    "content": response.choices[0].message.content,
                    "model": response.model,
                    "usage": response.usage.model_dump() if response.usage else {},
                    "latency": latency
                }

            # è®°å½•åˆ° Langfuse
            self._log_to_langfuse(
                messages=messages,
                response=response_data,
                session_id=session_id,
                user_id=user_id,
                metadata=metadata or {}
            )

            return response_data

        except Exception as e:
            end_time = time.time()
            latency = end_time - start_time

            # è®°å½•é”™è¯¯åˆ° Langfuse
            self._log_error_to_langfuse(
                messages=messages,
                error=str(e),
                latency=latency,
                session_id=session_id,
                user_id=user_id,
                metadata=metadata or {}
            )

            raise e

    def _log_to_langfuse(self,
                        messages: list,
                        response: Dict[str, Any],
                        session_id: Optional[str] = None,
                        user_id: Optional[str] = None,
                        metadata: Dict[str, Any] = None):
        """è®°å½•æˆåŠŸçš„è°ƒç”¨åˆ° Langfuse"""
        try:
            # åˆ›å»ºç”Ÿæˆè®°å½•
            generation = self.langfuse.generation(
                name="vllm_chat_completion",
                model=self.model_id,
                input=messages,
                output=response["content"],
                usage={
                    "input": sum(len(msg.get("content", "").split()) for msg in messages),
                    "output": response["usage"].get("completion_tokens", 0),
                    "total": response["usage"].get("total_tokens", 0)
                },
                metadata={
                    **metadata,
                    "base_url": self.base_url,
                    "max_tokens": self.max_tokens,
                    "temperature": self.temperature,
                    "latency": response["latency"]
                },
                session_id=session_id,
                user_id=user_id
            )

            print(f"âœ… æˆåŠŸè®°å½•åˆ° Langfuse: {generation.id}")

        except Exception as e:
            print(f"âŒ è®°å½•åˆ° Langfuse å¤±è´¥: {e}")

    def _log_error_to_langfuse(self,
                              messages: list,
                              error: str,
                              latency: float,
                              session_id: Optional[str] = None,
                              user_id: Optional[str] = None,
                              metadata: Dict[str, Any] = None):
        """è®°å½•é”™è¯¯çš„è°ƒç”¨åˆ° Langfuse"""
        try:
            # åˆ›å»ºé”™è¯¯è®°å½•
            generation = self.langfuse.generation(
                name="vllm_chat_completion_error",
                model=self.model_id,
                input=messages,
                output=f"ERROR: {error}",
                metadata={
                    **metadata,
                    "base_url": self.base_url,
                    "max_tokens": self.max_tokens,
                    "temperature": self.temperature,
                    "latency": latency,
                    "error": error
                },
                session_id=session_id,
                user_id=user_id,
                level="ERROR"
            )

            print(f"âŒ é”™è¯¯è®°å½•åˆ° Langfuse: {generation.id}")

        except Exception as e:
            print(f"âŒ è®°å½•é”™è¯¯åˆ° Langfuse å¤±è´¥: {e}")

    def simple_chat(self, prompt: str, **kwargs) -> str:
        """ç®€å•çš„èŠå¤©æ¥å£"""
        messages = [{"role": "user", "content": prompt}]
        response = self.chat_completion(messages, **kwargs)
        return response["content"]

    def flush_langfuse(self):
        """åˆ·æ–° Langfuse ç¼“å†²åŒºï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½è¢«å‘é€"""
        self.langfuse.flush()
        print("âœ… Langfuse æ•°æ®å·²åˆ·æ–°")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»º VLLM å®¢æˆ·ç«¯
    vllm_client = VLLMWithLangfuse(
        model_id='Qwen3-Omni-Thinking',
        base_url='http://223.109.239.14:10026/v1/',
        max_tokens=32768,
        temperature=1.0
    )

    # æµ‹è¯•å¯¹è¯
    print("\nğŸ¤– å¼€å§‹æµ‹è¯•å¯¹è¯...")

    try:
        # ç®€å•å¯¹è¯æµ‹è¯•
        response = vllm_client.simple_chat(
            "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
            session_id="test_session_001",
            user_id="test_user",
            metadata={"test_type": "simple_chat", "version": "1.0"}
        )

        print(f"\nğŸ¤– AI å›å¤: {response}")

        # å¤šè½®å¯¹è¯æµ‹è¯•
        messages = [
            {"role": "user", "content": "è¯·å‘Šè¯‰æˆ‘ä»Šå¤©çš„å¤©æ°”å¦‚ä½•ï¼Ÿ"},
            {"role": "assistant", "content": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è·å–å®æ—¶å¤©æ°”ä¿¡æ¯ã€‚"},
            {"role": "user", "content": "é‚£ä½ èƒ½åšä»€ä¹ˆå‘¢ï¼Ÿ"}
        ]

        response = vllm_client.chat_completion(
            messages=messages,
            session_id="test_session_002",
            user_id="test_user",
            metadata={"test_type": "multi_turn", "version": "1.0"}
        )

        print(f"\nğŸ¤– å¤šè½®å¯¹è¯å›å¤: {response['content']}")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

    finally:
        # ç¡®ä¿æ•°æ®è¢«å‘é€åˆ° Langfuse
        vllm_client.flush_langfuse()
        print("\nâœ… æµ‹è¯•å®Œæˆï¼Œæ•°æ®å·²è®°å½•åˆ° Langfuse")
